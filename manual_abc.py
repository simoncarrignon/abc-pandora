#!/usr/bin/python3

import csv, math, sys, argparse, random,os,errno,re
import numpy as np
import logging
import time
import subprocess
from ceecexp import Experiment
from ceecexp import order

class TophatPrior(object):
    """
    Tophat prior
    
    :param min: scalar or array of min values
    :param max: scalar or array of max values
    """
    
    def __init__(self, min, max):
        self.min = np.atleast_1d(min)
        self.max = np.atleast_1d(max)
        self._random = np.random.mtrand.RandomState()
        assert self.min.shape == self.max.shape
        assert np.all(self.min < self.max)
        
    def __call__(self, theta=None):
        if theta is None:
            return np.array([self._random.uniform(mi, ma) for (mi, ma) in zip(self.min, self.max)])
        else:
            return 1 if np.all(theta < self.max) and np.all(theta >= self.min) else 0




#generate a pool of experiment of size `size` that will be stored in the folder `pref`
def genTestPool(size,pref):
    pool_exp={}
    for p in range(size):
        priors = TophatPrior([0,0.5,0,750,1],[1,15,10,7500,30])
        params=priors()
        one=Experiment(params,"/home/bsc21/bsc21394/ceeculture/",pref)
	while(not one.consistence):
            params=priors()
	    one=Experiment(params,"/home/bsc21/bsc21394/ceeculture/",pref)
        pool_exp[one.getId()]=one
    return(pool_exp)



###Write the task file and update the counter of the number of task per file
def writeNupdate(tmp_pdict):

    ###ALL that would  be  really  nicer if ABC was an object 
    global countExpKind
    global countFileKind
    global tasks
    global numproc_node #defined given MN configuration
    global jobid

    
    for pone in tmp_pdict.keys() :
        one=tmp_pdict[pone]
        kind=one.getKind()
        task=one.generateTask()

        if( not( kind in countExpKind.keys())): #check if this kind is already recorded
            countExpKind[kind]=0 
            countFileKind[kind]=0

        countExpKind[kind]=countExpKind[kind]+1 #increase number of expe of this kind

        if(countExpKind[kind] > numproc_node): #if number of expe is too high, increase number of file 
            #TODO here should launch the file already full fullfillfillfull
            countFileKind[kind]=countFileKind[kind]+1
            countExpKind[kind]=0


        if (not os.path.isdir("taskfiles")):
            os.makedirs("taskfiles") #create folder for the taskfiles

	taskid=kind+"_"+str(countFileKind[kind])
        taskfilename=taskid+"-"+jobid+".task"
        taskfilename=os.path.join("taskfiles",taskfilename)

        with open(taskfilename,'a') as tskf:
            tskf.write(task)

        tasks[taskid]={}
        tasks[taskid]['filename']=taskfilename
        tasks[taskid]['status']='hold'
	logging.debug(tasks)

###launch batch of experiments given the machine used
#TODO a real class "launcher" that can abstract that from the ABC
def launchExpe(taskfile):
    time="00:05:00"
    if(os.getenv('BSC_MACHINE') == 'mn4'):
        command = "bash 2mn4.sh"
    if(os.getenv('BSC_MACHINE') == 'nord3'):
        command = "bash 2nord3.sh"
	
    command = " ".join([command,taskfile,time,str(numproc_node)])
    process = subprocess.Popen(command, stdout=subprocess.PIPE,shell=True)
    return(process)


## Write  a dictionnary of particules `particules` for the epsilon `epsi` in the file `outfilename`
#TODO set a better handling of the header
def writeParticules(particules,epsi,outfilename):
        sep=","
	with open(outfilename, 'wb') as outpart:
            header=order+sep+"score"+sep+'epsilon'+"\n"
       	    outpart.write(header)
    	    for eid, score in particules.items():
                thetas=eid.replace("_",",")
                row=thetas+ sep+ str(score) + sep + str(epsilon)+ "\n"
       	    	outpart.write(row)

########################################
#MAIN PROGRAM
# use:
#python ./manual_abc.py numParticule numpart numproc_node epsilon
#where :
#* `numParticule`: total number of  particule (aka Thetas, aka set of parameter) that you to draw your distribution (the bigger, the better)
#* `numpart`: number of particules we will generate and check at the same 
#* `numproc_node`: number of parallele task we run in one marenostrum node  (should be < numpart as the idea is , from the numpart generated by the python script we split and  run them in separated nodes)
#* `epsilon`: the maximum score we accept (o minimum) for the particule. _ie_ all particule should have a score < epsilon
#



if __name__ == '__main__' :
    pdict={}     #list of score for each exp
    countExpKind={} #number of experiment for each different "kind" 
    countFileKind={} #number of tasksfile for each different "kind" 
    tasks={} #list of taskfiles that have to be send to mn, it allows to separate the experiments in different sbatch given some conditions

    tmp_pdict={} #pool of particules
    numParticule=int(sys.argv[1]) #This is the total number of  particule (aka Thetas, aka set of parameter) that we want
    numproc=int(sys.argv[2]) #this is the number of parallele task we will try
    numproc_node=int(sys.argv[3]) #this is the number of parallele task we will try
    epsilon=float(sys.argv[4])  #the maximum score we accept (o minimum)

    orign=os.getcwd() #original workind directory
    
    pref="eps_"+str(epsilon)
    jobid="mother_"
    jobid+=str(os.getpid())
    try:
        jobid+="_"+os.getenv("SLURM_JOBID")
    except:
	print('not a slurm job')

    #open a general log file
    logging.basicConfig(format="%(asctime)s;%(levelname)s;%(message)s",filename=str(jobid)+".log",level=logging.INFO)
   
    tmp_pdict=genTestPool(numproc,pref)

    ###initialize pool
    writeNupdate(tmp_pdict)

    ##findFileneNameAndUpdateCounter
    #Launch remaining tasks
       
    oldlen=0
    while(len(pdict) < numParticule):

	if(len(pdict)>oldlen): ##logging only if new particules found
	    oldlen=len(pdict)
	    logging.info(str(len(pdict))+ "/"+str(numParticule)+ " tot")

        tsks=list(tasks.keys())

	dead=0
	for tid,tproc in tasks.items():
		##check status of the task
		#if on hold it means it has been created during previous loop and has to be launched
		if(tasks[tid]['status'] == 'hold'):
                	launcher=launchExpe(tasks[tid]['filename'])
			out, err = launcher.communicate()
			logging.info(out)
	 		try:
			    remote_id=re.search('Submitted batch job ([0-9]+)\n',out).group(1)
			    tasks[tid]['status'] = 'running'
			    tasks[tid]['remote_id'] = remote_id
			except:
			    logging.warning("Task ID not found")
			    tasks[tid]['status'] = 'error'
			    logging.warning('probleme while launching the job')

	       	#if the task is running (meaning a greasy job has been launched) we check if the job is still running
		# by looking at its status in the queue
	       	if(tasks[tid]['status'] == 'running'):
			command=""
			if(os.getenv('BSC_MACHINE') == 'mn4'):
				    command += "squeue -h -j"+tasks[tid]['remote_id']
    				    process = subprocess.Popen(command, stdout=subprocess.PIPE,shell=True)
				    out, err = process.communicate()
				    if(out == ''):
					logging.warning("taks "+tasks[tid]['remote_id']+" not running")
					tasks[tid]['status']="dead"
		
		##in every other case it means that the task ended so we should move on and start a new one
	       	if(tasks[tid]['status'] != 'running' and tasks[tid]['status'] != 'hold'):
			dead+=1	

        ##update the pool of particule given their score if the experiment has finished
        tmp_keys=list(tmp_pdict.keys())
        for t in tmp_keys:
            tmp_exp=tmp_pdict[t]
            tmp_exp.gatherScore()
            if(tmp_exp.score>0):
                if(tmp_exp.score > epsilon):
                    tmp_exp.remove()
                    tmp_pdict.pop(t,None)
                else:
                    pdict[tmp_exp.getId()]=tmp_exp.score
                    tmp_pdict.pop(t,None)

        #(the pool is empty ) all simulation finished and we have not yet enough particle
        if(len(pdict) < numParticule and dead == len(tasks)): 
	    logging.info("regenerate new taskfiles")
            ###re-initialize pool
            tmp_pdict=genTestPool(numproc,pref)
            writeNupdate(tmp_pdict)
            ##findFileneNameAndUpdateCounter
            #Launch remaining tasks
    writeParticules(pdict,epsilon,"result_"+str(epsilon)+".csv")
    logging.info('send cancel signal to remaining tasks')
    for tid,tproc in tasks.items():
	if(tasks[tid]['status'] == 'running'):
	    command="scancel "+tasks[tid]['remote_id']
	    process = subprocess.Popen(command, stdout=subprocess.PIPE,shell=True)
	    out, err = process.communicate()
	    logging.info('force: '+tasks[tid]['remote_id']+" to stop. ")
    logging.info('ABC done for epsilon'+str(epsilon))
